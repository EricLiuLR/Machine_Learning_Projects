{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train.shape: (1017209, 9)\n",
      "test.shape: (41088, 8)\n",
      "store.shape: (1115, 10)\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('/kaggle/input/rossmann-store-sales/train.csv', parse_dates=['Date'])\n",
    "test = pd.read_csv('/kaggle/input/rossmann-store-sales/test.csv', parse_dates=['Date'])\n",
    "store = pd.read_csv('/kaggle/input/rossmann-store-sales/store.csv')\n",
    "\n",
    "print('train.shape: {}'.format(train.shape))\n",
    "print('test.shape: {}'.format(test.shape))\n",
    "print('store.shape: {}'.format(store.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the store table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Competition date information to datetime for comparing\n",
    "def convert_competition_open(row):\n",
    "    try:\n",
    "        date = '{}-{}'.format(int(row['CompetitionOpenSinceYear']), int(row['CompetitionOpenSinceMonth']))\n",
    "        return pd.to_datetime(date)\n",
    "    except:\n",
    "        return np.nan\n",
    "store['CompetitionOpen'] = store.apply(convert_competition_open, axis=1)\n",
    "store = store.drop(['CompetitionOpenSinceYear', 'CompetitionOpenSinceMonth'], axis=1)\n",
    "\n",
    "# Convert Promo2 information to datetime for comparing\n",
    "def convert_promo2(row):\n",
    "    try:\n",
    "        date = '{}{}1'.format(int(row['Promo2SinceYear']), int(row['Promo2SinceWeek']))\n",
    "        return pd.to_datetime(date, format='%Y%W%w')\n",
    "    except:\n",
    "        return np.nan\n",
    "store['Promo2Since'] = store.apply(convert_promo2, axis=1)\n",
    "store = store.drop(['Promo2', 'Promo2SinceYear', 'Promo2SinceWeek'], axis=1)\n",
    "\n",
    "# Add 12 attributes of the months each store is running promo2\n",
    "months = {\n",
    "    'Jan': 1,\n",
    "    'Feb' : 2,\n",
    "    'Mar' : 3,\n",
    "    'Apr' : 4,\n",
    "    'May' : 5,\n",
    "    'Jun' : 6,\n",
    "    'Jul' : 7,\n",
    "    'Aug' : 8,\n",
    "    'Sept' : 9, \n",
    "    'Oct' : 10,\n",
    "    'Nov' : 11,\n",
    "    'Dec' : 12\n",
    "}\n",
    "def add_promo2_month(interval, month):\n",
    "    if pd.isnull(interval):\n",
    "        return np.nan\n",
    "    else:\n",
    "        if month in interval.split(','):\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "for month in months.keys():\n",
    "    store['Promo2_on_month' + '_' + str(months[month])] = store.PromoInterval.apply(add_promo2_month, args=(month,))\n",
    "store = store.drop('PromoInterval', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1115 entries, 0 to 1114\n",
      "Data columns (total 18 columns):\n",
      "Store                  1115 non-null int64\n",
      "StoreType              1115 non-null object\n",
      "Assortment             1115 non-null object\n",
      "CompetitionDistance    1112 non-null float64\n",
      "CompetitionOpen        761 non-null datetime64[ns]\n",
      "Promo2Since            571 non-null datetime64[ns]\n",
      "Promo2_on_month_1      571 non-null float64\n",
      "Promo2_on_month_2      571 non-null float64\n",
      "Promo2_on_month_3      571 non-null float64\n",
      "Promo2_on_month_4      571 non-null float64\n",
      "Promo2_on_month_5      571 non-null float64\n",
      "Promo2_on_month_6      571 non-null float64\n",
      "Promo2_on_month_7      571 non-null float64\n",
      "Promo2_on_month_8      571 non-null float64\n",
      "Promo2_on_month_9      571 non-null float64\n",
      "Promo2_on_month_10     571 non-null float64\n",
      "Promo2_on_month_11     571 non-null float64\n",
      "Promo2_on_month_12     571 non-null float64\n",
      "dtypes: datetime64[ns](2), float64(13), int64(1), object(2)\n",
      "memory usage: 156.9+ KB\n"
     ]
    }
   ],
   "source": [
    "store.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop the outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all the rows with store open but zero sales\n",
    "train = train.drop(train[(train.Sales==0) & (train.Open==1)].index, axis=0)\n",
    "\n",
    "# For each store, drop the abnormal sales\n",
    "stats = train.groupby('Store').Sales.agg(['mean', 'std']).reset_index()\n",
    "sales = train[['Store', 'Sales']].copy().reset_index()\n",
    "sales = pd.merge(sales, stats, on='Store')\n",
    "sales['z_score'] = (sales['Sales'] - sales['mean']) / sales['std']\n",
    "\n",
    "index_to_drop = sales.loc[sales.z_score>=3, 'index']\n",
    "train = train.drop(index_to_drop, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine the data for easy processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['istestset'] = 0\n",
    "test['istestset'] = 1\n",
    "combine = pd.concat([train.drop(['Sales', 'Customers'], axis=1), test.drop('Id', axis=1)], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add the store table information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine = pd.merge(combine, store, how='left', on='Store')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill in missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in Open with mode\n",
    "combine['Open'] = combine.Open.fillna(1.0)\n",
    "\n",
    "# Fill in all the promo2 related attributes with -1, which indicates a special case(not participate in promo2)\n",
    "promo2_related_feats = [_ for _ in combine.columns if 'Promo2' in _]\n",
    "for feat in promo2_related_feats:\n",
    "    combine[feat] = combine[feat].fillna(-1)\n",
    "    \n",
    "# Fill in the missing values in CompetitionDistance with the max + 1000, which means no nearby competition\n",
    "combine['CompetitionDistance'] = combine['CompetitionDistance'].fillna(combine.CompetitionDistance.max() + 1000)\n",
    "\n",
    "# For the missing values in CompetitionOpen, I just chenge it to categorical attribute (with 0 means not open yet, 1 means opened, -1 means unclear)\n",
    "def convert_CompetitionOpen_cat(row):\n",
    "    if pd.isnull(row['CompetitionOpen']):\n",
    "        return -1\n",
    "    else:\n",
    "        if row['Date'] >= row['CompetitionOpen']:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "combine['CompetitionOpen'] = combine.apply(convert_CompetitionOpen_cat, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DayOfWeek\n",
    "combine['DayOfWeek'] = combine.DayOfWeek.astype(int)\n",
    "\n",
    "# Date\n",
    "combine['Date'] = pd.to_datetime(combine.Date)\n",
    "combine['Year'] = combine.Date.dt.year.astype(int)\n",
    "combine['Month'] = combine.Date.dt.month.astype(int)\n",
    "combine['Day'] = combine.Date.dt.day.astype(int)\n",
    "combine['Week'] = combine.Date.dt.week.astype(int)\n",
    "\n",
    "# Open ...\n",
    "# Promo ...\n",
    "\n",
    "# StateHoliday\n",
    "combine['StateHoliday'] = combine.StateHoliday.replace({0:'0'}).astype('category').cat.codes\n",
    "\n",
    "# SchoolHoliday ...\n",
    "\n",
    "# StoreType\n",
    "combine['StoreType'] = combine.StoreType.astype('category').cat.codes\n",
    "\n",
    "# Assortment\n",
    "combine['Assortment'] = combine.Assortment.astype('category').cat.codes\n",
    "\n",
    "# Promo2 conver into catigorical feature(with 0 means not on, 1 means on, -1 means not participate)\n",
    "def create_promo2(row):\n",
    "    if row['Promo2Since'] == -1:\n",
    "        return -1\n",
    "    else:\n",
    "        month = row['Month']\n",
    "        if row['Promo2_on_month_' + str(month)] == 1 and row['Date'] > row['Promo2Since']:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "combine['Promo2'] = combine.apply(create_promo2, axis=1)\n",
    "combine = combine.drop(promo2_related_feats, axis=1)\n",
    "\n",
    "combine = combine.drop('Date', axis=1)\n",
    "\n",
    "combine = pd.get_dummies(combine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the data for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = combine[combine.istestset==0].drop('istestset', axis=1)\n",
    "label_train = train['Sales']\n",
    "label_train_log = np.log1p(train['Sales'])\n",
    "data_test = combine[combine.istestset==1].drop('istestset', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the basic setting of the model and Set up the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from lightgbm.sklearn import LGBMRegressor\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import make_scorer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I tried the following 4 settings for the model and chose the best:\n",
    "\n",
    "- Default objective function without taking log of the target\n",
    "- Default objective function with taking log of the target\n",
    "- Custom objective function without taking log of the target\n",
    "- Custom objective function with taking log of the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_eval, y_train, y_eval = train_test_split(data_train, label_train, test_size=0.1, random_state=0)\n",
    "X_train, X_eval, y_train_log, y_eval_log = train_test_split(data_train, label_train_log, test_size=0.1, random_state=0)\n",
    "\n",
    "dataset_train = lgb.Dataset(X_train, y_train)\n",
    "dataset_eval = lgb.Dataset(X_eval, y_eval, reference=dataset_train)\n",
    "\n",
    "dataset_train_log = lgb.Dataset(X_train, y_train_log)\n",
    "dataset_eval_log = lgb.Dataset(X_eval, y_eval_log, reference=dataset_train_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rounds = 2000\n",
    "verb_rounds = 200\n",
    "earlystop_rounds = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default objective function without taking log of the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\ttraining set's rmspe: 0.294888\tvalidation set's rmspe: 0.268289\n",
      "[400]\ttraining set's rmspe: 0.241588\tvalidation set's rmspe: 0.213323\n",
      "[600]\ttraining set's rmspe: 0.217571\tvalidation set's rmspe: 0.189241\n",
      "[800]\ttraining set's rmspe: 0.205693\tvalidation set's rmspe: 0.175445\n",
      "[1000]\ttraining set's rmspe: 0.19638\tvalidation set's rmspe: 0.165255\n",
      "[1200]\ttraining set's rmspe: 0.190344\tvalidation set's rmspe: 0.159234\n",
      "[1400]\ttraining set's rmspe: 0.185443\tvalidation set's rmspe: 0.154303\n",
      "[1600]\ttraining set's rmspe: 0.181671\tvalidation set's rmspe: 0.15063\n",
      "[1800]\ttraining set's rmspe: 0.178757\tvalidation set's rmspe: 0.148736\n",
      "[2000]\ttraining set's rmspe: 0.175992\tvalidation set's rmspe: 0.146256\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2000]\ttraining set's rmspe: 0.175992\tvalidation set's rmspe: 0.146256\n"
     ]
    }
   ],
   "source": [
    "def loss(y_pred, y):\n",
    "    y = y.get_label()\n",
    "    n = y.shape[0]\n",
    "    ind = (y!=0)\n",
    "    y_pred, y = y_pred[ind], y[ind]\n",
    "    dis_percent_squared = ((y - y_pred) / y) ** 2\n",
    "    loss = np.sqrt(dis_percent_squared.sum() / n)\n",
    "    return 'rmspe', loss, False\n",
    "\n",
    "bst = lgb.train({}, dataset_train, num_rounds, \n",
    "          valid_sets=[dataset_train, dataset_eval], valid_names=['training set', 'validation set'], \n",
    "          feval=loss, verbose_eval=verb_rounds, early_stopping_rounds=earlystop_rounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default objective function with taking log of the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\ttraining set's rmspe_log: 0.244215\tvalidation set's rmspe_log: 0.222901\n",
      "[400]\ttraining set's rmspe_log: 0.204562\tvalidation set's rmspe_log: 0.180372\n",
      "[600]\ttraining set's rmspe_log: 0.186844\tvalidation set's rmspe_log: 0.162092\n",
      "[800]\ttraining set's rmspe_log: 0.177329\tvalidation set's rmspe_log: 0.151472\n",
      "[1000]\ttraining set's rmspe_log: 0.168835\tvalidation set's rmspe_log: 0.144647\n",
      "[1200]\ttraining set's rmspe_log: 0.162481\tvalidation set's rmspe_log: 0.140195\n",
      "[1400]\ttraining set's rmspe_log: 0.157999\tvalidation set's rmspe_log: 0.137242\n",
      "[1600]\ttraining set's rmspe_log: 0.154079\tvalidation set's rmspe_log: 0.134373\n",
      "[1800]\ttraining set's rmspe_log: 0.148071\tvalidation set's rmspe_log: 0.132366\n",
      "[2000]\ttraining set's rmspe_log: 0.146299\tvalidation set's rmspe_log: 0.130339\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2000]\ttraining set's rmspe_log: 0.146299\tvalidation set's rmspe_log: 0.130339\n"
     ]
    }
   ],
   "source": [
    "def loss_log(y_pred, y):\n",
    "    y = y.get_label()\n",
    "    n = y.shape[0]\n",
    "    y_pred = np.exp(y_pred) - 1\n",
    "    y = np.exp(y) - 1\n",
    "    ind = (y!=0)\n",
    "    y_pred, y = y_pred[ind], y[ind]\n",
    "    dis_percent_squared = ((y - y_pred) / y) ** 2\n",
    "    loss = np.sqrt(dis_percent_squared.sum() / n)\n",
    "    return 'rmspe_log', loss, False\n",
    "\n",
    "bst = lgb.train({}, dataset_train_log, num_rounds, \n",
    "          valid_sets = [dataset_train_log, dataset_eval_log], valid_names=['training set', 'validation set'], \n",
    "          feval=loss_log, verbose_eval=verb_rounds, early_stopping_rounds=earlystop_rounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom objective function without taking log of the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\ttraining set's rmspe: 0.237106\tvalidation set's rmspe: 0.242441\n",
      "[400]\ttraining set's rmspe: 0.211457\tvalidation set's rmspe: 0.21837\n",
      "[600]\ttraining set's rmspe: 0.197416\tvalidation set's rmspe: 0.205294\n",
      "[800]\ttraining set's rmspe: 0.187602\tvalidation set's rmspe: 0.196257\n",
      "[1000]\ttraining set's rmspe: 0.180059\tvalidation set's rmspe: 0.189421\n",
      "[1200]\ttraining set's rmspe: 0.174244\tvalidation set's rmspe: 0.183894\n",
      "[1400]\ttraining set's rmspe: 0.169223\tvalidation set's rmspe: 0.179328\n",
      "[1600]\ttraining set's rmspe: 0.165168\tvalidation set's rmspe: 0.175633\n",
      "[1800]\ttraining set's rmspe: 0.161698\tvalidation set's rmspe: 0.172382\n",
      "[2000]\ttraining set's rmspe: 0.15853\tvalidation set's rmspe: 0.169646\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2000]\ttraining set's rmspe: 0.15853\tvalidation set's rmspe: 0.169646\n"
     ]
    }
   ],
   "source": [
    "def rmspe_loss(y_pred, y):\n",
    "    y = y.get_label()\n",
    "    dis = y - y_pred\n",
    "    ind = (dis!=0) & (y!=0)\n",
    "    gradient = np.zeros(y.shape)\n",
    "    hession = np.zeros(y.shape)\n",
    "    n = y.shape[0]\n",
    "    gradient[ind] = 2 * (y_pred[ind] - y[ind]) / y[ind] ** 2\n",
    "    hession[ind] = 2 / y[ind] ** 2\n",
    "    return gradient, hession\n",
    "\n",
    "bst = lgb.train({}, dataset_train, num_rounds, \n",
    "          valid_sets = [dataset_train, dataset_eval], \n",
    "          valid_names=['training set', 'validation set'], \n",
    "          fobj=rmspe_loss, feval=loss, \n",
    "          verbose_eval=verb_rounds, early_stopping_rounds=earlystop_rounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom objective function with taking log of the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[200]\ttraining set's rmspe_log: 0.223255\tvalidation set's rmspe_log: 0.200649\n",
      "[400]\ttraining set's rmspe_log: 0.176031\tvalidation set's rmspe_log: 0.163059\n",
      "[600]\ttraining set's rmspe_log: 0.155076\tvalidation set's rmspe_log: 0.147874\n",
      "[800]\ttraining set's rmspe_log: 0.137531\tvalidation set's rmspe_log: 0.139298\n",
      "[1000]\ttraining set's rmspe_log: 0.127034\tvalidation set's rmspe_log: 0.136196\n",
      "[1200]\ttraining set's rmspe_log: 0.116847\tvalidation set's rmspe_log: 0.132746\n",
      "[1400]\ttraining set's rmspe_log: 0.111347\tvalidation set's rmspe_log: 0.13031\n",
      "[1600]\ttraining set's rmspe_log: 0.108136\tvalidation set's rmspe_log: 0.128519\n",
      "[1800]\ttraining set's rmspe_log: 0.105072\tvalidation set's rmspe_log: 0.12749\n",
      "[2000]\ttraining set's rmspe_log: 0.102408\tvalidation set's rmspe_log: 0.126228\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2000]\ttraining set's rmspe_log: 0.102408\tvalidation set's rmspe_log: 0.126228\n"
     ]
    }
   ],
   "source": [
    "def rmspe_loss(y_pred, y):\n",
    "    y = y.get_label()\n",
    "    dis = y - y_pred\n",
    "    ind = (dis!=0) & (y!=0)\n",
    "    gradient = np.zeros(y.shape)\n",
    "    hession = np.zeros(y.shape)\n",
    "    n = y.shape[0]\n",
    "    gradient[ind] = 2 * (y_pred[ind] - y[ind]) / y[ind] ** 2\n",
    "    hession[ind] = 2 / y[ind] ** 2\n",
    "    return gradient, hession\n",
    "\n",
    "bst = lgb.train({}, dataset_train_log, num_rounds, \n",
    "          valid_sets = [dataset_train_log, dataset_eval_log], \n",
    "          valid_names=['training set', 'validation set'], \n",
    "          fobj=rmspe_loss, feval=loss_log, \n",
    "          verbose_eval=verb_rounds, early_stopping_rounds=earlystop_rounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Since the custom objective function with taking log performs the best, I choose this setting**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gridsearch for best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KFold for consistant cv:\n",
    "kf = KFold(n_splits=4, shuffle=True, random_state=0)\n",
    "\n",
    "# Custom scoring function\n",
    "def loss_log_sk(y, y_pred):\n",
    "    n = y.shape[0]\n",
    "    y_pred = np.exp(y_pred) - 1\n",
    "    y = np.exp(y) - 1\n",
    "    ind = (y!=0)\n",
    "    y_pred, y = y_pred[ind], y[ind]\n",
    "    dis_percent_squared = ((y - y_pred) / y) ** 2\n",
    "    loss = np.sqrt(dis_percent_squared.sum() / n)\n",
    "    return loss\n",
    "mt = make_scorer(loss_log_sk, greater_is_better=False)\n",
    "\n",
    "# Simple performance measure function\n",
    "def performance(model):\n",
    "    scores = cross_val_score(model, data_train, label_train_log, cv=kf, scoring=mt, n_jobs=4, verbose=True)\n",
    "    score_mean = scores.mean()\n",
    "    score_std = scores.std()\n",
    "    print('score mean: {}'.format(-score_mean))\n",
    "    print('score std: {}'.format(score_std))\n",
    "\n",
    "# Grid search function\n",
    "def grid_search(model, params):\n",
    "    grid = GridSearchCV(model, params, cv=kf, scoring=mt, verbose=True).fit(data_train, label_train_log)\n",
    "    print('grid.best_score_: {}'.format(-grid.best_score_))\n",
    "    print('grid.best_params_: \\n{}'.format(grid.best_params_))\n",
    "    return grid.best_estimator_\n",
    "\n",
    "# Custom objective function for sklearn api\n",
    "def rmspe_loss_sk(y, y_pred):\n",
    "    dis = y - y_pred\n",
    "    ind = (dis!=0) & (y!=0)\n",
    "    gradient = np.zeros(y.shape)\n",
    "    hession = np.zeros(y.shape)\n",
    "    n = y.shape[0]\n",
    "    gradient[ind] = 2 * (y_pred[ind] - y[ind]) / y[ind] ** 2\n",
    "    hession[ind] = 2 / y[ind] ** 2\n",
    "    return gradient, hession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 1 candidates, totalling 4 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:   55.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grid.best_score_: 0.15277929148837271\n",
      "grid.best_params_: \n",
      "{'colsample_bytree': 1.0, 'learning_rate': 0.7, 'max_depth': -1, 'n_estimators': 100, 'num_leaves': 315, 'objective': <function rmspe_loss_sk at 0x7f38c222c8c8>, 'random_state': 0, 'reg_alpha': 1e-07, 'reg_lambda': 1e-05, 'subsample': 0.7}\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    'random_state': [0],\n",
    "    'n_estimators': [100],\n",
    "    'learning_rate': [0.7],\n",
    "    'objective': [rmspe_loss_sk],\n",
    "    'num_leaves': [315],\n",
    "    'max_depth': [-1],\n",
    "    'reg_alpha': [1e-7],\n",
    "    'reg_lambda': [1e-5],\n",
    "    'subsample': [0.7],\n",
    "    'colsample_bytree': [1.0]\n",
    "}\n",
    "reg = grid_search(LGBMRegressor(), params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set small learning_rate, then use early stopping to find the optimal iteration rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 300 rounds.\n",
      "[500]\ttraining set's rmspe_log: 0.174575\tvalidation set's rmspe_log: 0.171556\n",
      "[1000]\ttraining set's rmspe_log: 0.124728\tvalidation set's rmspe_log: 0.140717\n",
      "[1500]\ttraining set's rmspe_log: 0.108971\tvalidation set's rmspe_log: 0.13233\n",
      "[2000]\ttraining set's rmspe_log: 0.101931\tvalidation set's rmspe_log: 0.128243\n",
      "[2500]\ttraining set's rmspe_log: 0.0966869\tvalidation set's rmspe_log: 0.125934\n",
      "[3000]\ttraining set's rmspe_log: 0.0929191\tvalidation set's rmspe_log: 0.124043\n",
      "[3500]\ttraining set's rmspe_log: 0.0901291\tvalidation set's rmspe_log: 0.122513\n",
      "[4000]\ttraining set's rmspe_log: 0.087874\tvalidation set's rmspe_log: 0.121311\n",
      "[4500]\ttraining set's rmspe_log: 0.085884\tvalidation set's rmspe_log: 0.120109\n",
      "[5000]\ttraining set's rmspe_log: 0.0842569\tvalidation set's rmspe_log: 0.119263\n",
      "[5500]\ttraining set's rmspe_log: 0.0826492\tvalidation set's rmspe_log: 0.118661\n",
      "[6000]\ttraining set's rmspe_log: 0.0813986\tvalidation set's rmspe_log: 0.118189\n",
      "[6500]\ttraining set's rmspe_log: 0.0800204\tvalidation set's rmspe_log: 0.117428\n",
      "[7000]\ttraining set's rmspe_log: 0.0789532\tvalidation set's rmspe_log: 0.116807\n",
      "[7500]\ttraining set's rmspe_log: 0.0778762\tvalidation set's rmspe_log: 0.116125\n",
      "[8000]\ttraining set's rmspe_log: 0.0768284\tvalidation set's rmspe_log: 0.115749\n",
      "[8500]\ttraining set's rmspe_log: 0.0758158\tvalidation set's rmspe_log: 0.114805\n",
      "[9000]\ttraining set's rmspe_log: 0.0748281\tvalidation set's rmspe_log: 0.114498\n",
      "[9500]\ttraining set's rmspe_log: 0.0739593\tvalidation set's rmspe_log: 0.114286\n",
      "[10000]\ttraining set's rmspe_log: 0.0731683\tvalidation set's rmspe_log: 0.114206\n",
      "[10500]\ttraining set's rmspe_log: 0.0723723\tvalidation set's rmspe_log: 0.11407\n",
      "[11000]\ttraining set's rmspe_log: 0.0717011\tvalidation set's rmspe_log: 0.113489\n",
      "[11500]\ttraining set's rmspe_log: 0.0710546\tvalidation set's rmspe_log: 0.112981\n",
      "[12000]\ttraining set's rmspe_log: 0.0704143\tvalidation set's rmspe_log: 0.112646\n",
      "[12500]\ttraining set's rmspe_log: 0.0698434\tvalidation set's rmspe_log: 0.11219\n",
      "[13000]\ttraining set's rmspe_log: 0.069266\tvalidation set's rmspe_log: 0.111986\n",
      "[13500]\ttraining set's rmspe_log: 0.0687342\tvalidation set's rmspe_log: 0.111885\n",
      "[14000]\ttraining set's rmspe_log: 0.068256\tvalidation set's rmspe_log: 0.111801\n",
      "[14500]\ttraining set's rmspe_log: 0.0677553\tvalidation set's rmspe_log: 0.111638\n",
      "[15000]\ttraining set's rmspe_log: 0.0672886\tvalidation set's rmspe_log: 0.111564\n",
      "[15500]\ttraining set's rmspe_log: 0.0667922\tvalidation set's rmspe_log: 0.111369\n",
      "[16000]\ttraining set's rmspe_log: 0.0662975\tvalidation set's rmspe_log: 0.11123\n",
      "[16500]\ttraining set's rmspe_log: 0.065835\tvalidation set's rmspe_log: 0.111114\n",
      "[17000]\ttraining set's rmspe_log: 0.0653947\tvalidation set's rmspe_log: 0.111173\n",
      "Early stopping, best iteration is:\n",
      "[16716]\ttraining set's rmspe_log: 0.0656541\tvalidation set's rmspe_log: 0.111088\n"
     ]
    }
   ],
   "source": [
    "num_rounds = 20000\n",
    "verb_rounds = 500\n",
    "earlystop_rounds = 300\n",
    "\n",
    "params = {\n",
    "    'random_state': [0],\n",
    "    'learning_rate': [0.01],\n",
    "    'num_leaves': [315],\n",
    "    'max_depth': [-1],\n",
    "    'reg_alpha': [1e-7],\n",
    "    'reg_lambda': [1e-5],\n",
    "    'subsample': [0.7],\n",
    "    'colsample_bytree': [1.0]\n",
    "}\n",
    "\n",
    "reg = lgb.train(params, dataset_train_log, num_rounds,\n",
    "                valid_sets=[dataset_train_log, dataset_eval_log],\n",
    "                valid_names=['training set', 'validation set'],\n",
    "                fobj=rmspe_loss, feval=loss_log,\n",
    "                verbose_eval=verb_rounds, early_stopping_rounds=earlystop_rounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrain the model on the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 300 rounds.\n",
      "[500]\ttraining set's rmspe_log: 0.176323\n",
      "[1000]\ttraining set's rmspe_log: 0.126947\n",
      "[1500]\ttraining set's rmspe_log: 0.111427\n",
      "[2000]\ttraining set's rmspe_log: 0.103172\n",
      "[2500]\ttraining set's rmspe_log: 0.097354\n",
      "[3000]\ttraining set's rmspe_log: 0.0937061\n",
      "[3500]\ttraining set's rmspe_log: 0.0907031\n",
      "[4000]\ttraining set's rmspe_log: 0.0882972\n",
      "[4500]\ttraining set's rmspe_log: 0.086509\n",
      "[5000]\ttraining set's rmspe_log: 0.0849142\n",
      "[5500]\ttraining set's rmspe_log: 0.0833527\n",
      "[6000]\ttraining set's rmspe_log: 0.0821277\n",
      "[6500]\ttraining set's rmspe_log: 0.080944\n",
      "[7000]\ttraining set's rmspe_log: 0.0798273\n",
      "[7500]\ttraining set's rmspe_log: 0.0787102\n",
      "[8000]\ttraining set's rmspe_log: 0.0777794\n",
      "[8500]\ttraining set's rmspe_log: 0.0769674\n",
      "[9000]\ttraining set's rmspe_log: 0.0761108\n",
      "[9500]\ttraining set's rmspe_log: 0.075293\n",
      "[10000]\ttraining set's rmspe_log: 0.0745407\n",
      "[10500]\ttraining set's rmspe_log: 0.0738616\n",
      "[11000]\ttraining set's rmspe_log: 0.0731945\n",
      "[11500]\ttraining set's rmspe_log: 0.0725494\n",
      "[12000]\ttraining set's rmspe_log: 0.0719607\n",
      "[12500]\ttraining set's rmspe_log: 0.0713442\n",
      "[13000]\ttraining set's rmspe_log: 0.0707908\n",
      "[13500]\ttraining set's rmspe_log: 0.0702081\n",
      "[14000]\ttraining set's rmspe_log: 0.0696945\n",
      "[14500]\ttraining set's rmspe_log: 0.0691116\n",
      "[15000]\ttraining set's rmspe_log: 0.0685574\n",
      "[15500]\ttraining set's rmspe_log: 0.0680701\n",
      "[16000]\ttraining set's rmspe_log: 0.06764\n",
      "[16500]\ttraining set's rmspe_log: 0.0671838\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[16716]\ttraining set's rmspe_log: 0.0670087\n"
     ]
    }
   ],
   "source": [
    "num_iter = reg.best_iteration\n",
    "\n",
    "dataset = lgb.Dataset(data_train, label_train_log)\n",
    "reg = lgb.train(params, dataset, num_iter,\n",
    "                valid_sets=[dataset],\n",
    "                valid_names=['training set'],\n",
    "                fobj=rmspe_loss, feval=loss_log, \n",
    "                verbose_eval=verb_rounds, early_stopping_rounds=earlystop_rounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f38bdaa8a90>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "feature_importance = pd.DataFrame()\n",
    "feature_importance['feature'] = reg.feature_name()\n",
    "feature_importance['importance'] = reg.feature_importance()\n",
    "feature_importance = feature_importance.sort_values('importance', ascending=False)\n",
    "sns.barplot(data=feature_importance, x='importance', y='feature')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.exp(reg.predict(data_test)) - 1\n",
    "y_pred = pd.Series(y_pred, name='Sales')\n",
    "sub = pd.concat([test['Id'], y_pred], axis=1)\n",
    "sub.to_csv('sub.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
