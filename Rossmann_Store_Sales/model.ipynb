{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "np.set_printoptions(precision=4)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.width', 100)\n",
    "pd.set_option('precision', 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3058: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train.shape: (1017209, 9)\n",
      "test.shape: (41088, 8)\n",
      "store.shape: (1115, 10)\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('/kaggle/input/rossmann-store-sales/train.csv', parse_dates=['Date'])\n",
    "test = pd.read_csv('/kaggle/input/rossmann-store-sales/test.csv', parse_dates=['Date'])\n",
    "store = pd.read_csv('/kaggle/input/rossmann-store-sales/store.csv')\n",
    "\n",
    "print('train.shape: {}'.format(train.shape))\n",
    "print('test.shape: {}'.format(test.shape))\n",
    "print('store.shape: {}'.format(store.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the store table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all the rows with store open but zero sales\n",
    "train = train.drop(train[(train.Sales==0) & (train.Open==1)].index, axis=0)\n",
    "\n",
    "# For each store, year, and month, drop the abnormal sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Competition date information to datetime for comparing\n",
    "def convert_competition_open(row):\n",
    "    try:\n",
    "        date = '{}-{}'.format(int(row['CompetitionOpenSinceYear']), int(row['CompetitionOpenSinceMonth']))\n",
    "        return pd.to_datetime(date)\n",
    "    except:\n",
    "        return np.nan\n",
    "store['CompetitionOpen'] = store.apply(convert_competition_open, axis=1)\n",
    "store = store.drop(['CompetitionOpenSinceYear', 'CompetitionOpenSinceMonth'], axis=1)\n",
    "\n",
    "# Convert Promo2 information to datetime for comparing\n",
    "def convert_promo2(row):\n",
    "    try:\n",
    "        date = '{}{}1'.format(int(row['Promo2SinceYear']), int(row['Promo2SinceWeek']))\n",
    "        return pd.to_datetime(date, format='%Y%W%w')\n",
    "    except:\n",
    "        return np.nan\n",
    "store['Promo2Since'] = store.apply(convert_promo2, axis=1)\n",
    "store = store.drop(['Promo2', 'Promo2SinceYear', 'Promo2SinceWeek'], axis=1)\n",
    "\n",
    "# Add 12 attributes of the months each store is running promo2\n",
    "months = {\n",
    "    'Jan': 1,\n",
    "    'Feb' : 2,\n",
    "    'Mar' : 3,\n",
    "    'Apr' : 4,\n",
    "    'May' : 5,\n",
    "    'Jun' : 6,\n",
    "    'Jul' : 7,\n",
    "    'Aug' : 8,\n",
    "    'Sept' : 9, \n",
    "    'Oct' : 10,\n",
    "    'Nov' : 11,\n",
    "    'Dec' : 12\n",
    "}\n",
    "def add_promo2_month(interval, month):\n",
    "    if pd.isnull(interval):\n",
    "        return np.nan\n",
    "    else:\n",
    "        if month in interval.split(','):\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "for month in months.keys():\n",
    "    store['Promo2_on_month' + '_' + str(months[month])] = store.PromoInterval.apply(add_promo2_month, args=(month,))\n",
    "store = store.drop('PromoInterval', axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1115 entries, 0 to 1114\n",
      "Data columns (total 18 columns):\n",
      "Store                  1115 non-null int64\n",
      "StoreType              1115 non-null object\n",
      "Assortment             1115 non-null object\n",
      "CompetitionDistance    1112 non-null float64\n",
      "CompetitionOpen        761 non-null datetime64[ns]\n",
      "Promo2Since            571 non-null datetime64[ns]\n",
      "Promo2_on_month_1      571 non-null float64\n",
      "Promo2_on_month_2      571 non-null float64\n",
      "Promo2_on_month_3      571 non-null float64\n",
      "Promo2_on_month_4      571 non-null float64\n",
      "Promo2_on_month_5      571 non-null float64\n",
      "Promo2_on_month_6      571 non-null float64\n",
      "Promo2_on_month_7      571 non-null float64\n",
      "Promo2_on_month_8      571 non-null float64\n",
      "Promo2_on_month_9      571 non-null float64\n",
      "Promo2_on_month_10     571 non-null float64\n",
      "Promo2_on_month_11     571 non-null float64\n",
      "Promo2_on_month_12     571 non-null float64\n",
      "dtypes: datetime64[ns](2), float64(13), int64(1), object(2)\n",
      "memory usage: 156.9+ KB\n"
     ]
    }
   ],
   "source": [
    "store.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop the outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all the rows with store open but zero sales\n",
    "train = train.drop(train[(train.Sales==0) & (train.Open==1)].index, axis=0)\n",
    "\n",
    "# For each store, year, and month, drop the abnormal sales\n",
    "stats = train.groupby('Store').Sales.agg(['mean', 'std']).reset_index()\n",
    "sales = train[['Store', 'Sales']].copy().reset_index()\n",
    "sales = pd.merge(sales, stats, on='Store')\n",
    "sales['z_score'] = (sales['Sales'] - sales['mean']) / sales['std']\n",
    "\n",
    "index_to_drop = sales.loc[sales.z_score>=3, 'index']\n",
    "train = train.drop(index_to_drop, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine the data for easy processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['istestset'] = 0\n",
    "test['istestset'] = 1\n",
    "combine = pd.concat([train.drop(['Sales', 'Customers'], axis=1), test.drop('Id', axis=1)], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add the store table information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine = pd.merge(combine, store, how='left', on='Store')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1057131 entries, 0 to 1057130\n",
      "Data columns (total 25 columns):\n",
      "Store                  1057131 non-null int64\n",
      "DayOfWeek              1057131 non-null int64\n",
      "Date                   1057131 non-null datetime64[ns]\n",
      "Open                   1057120 non-null float64\n",
      "Promo                  1057131 non-null int64\n",
      "StateHoliday           1057131 non-null object\n",
      "SchoolHoliday          1057131 non-null int64\n",
      "istestset              1057131 non-null int64\n",
      "StoreType              1057131 non-null object\n",
      "Assortment             1057131 non-null object\n",
      "CompetitionDistance    1054398 non-null float64\n",
      "CompetitionOpen        718946 non-null datetime64[ns]\n",
      "Promo2Since            532356 non-null datetime64[ns]\n",
      "Promo2_on_month_1      532356 non-null float64\n",
      "Promo2_on_month_2      532356 non-null float64\n",
      "Promo2_on_month_3      532356 non-null float64\n",
      "Promo2_on_month_4      532356 non-null float64\n",
      "Promo2_on_month_5      532356 non-null float64\n",
      "Promo2_on_month_6      532356 non-null float64\n",
      "Promo2_on_month_7      532356 non-null float64\n",
      "Promo2_on_month_8      532356 non-null float64\n",
      "Promo2_on_month_9      532356 non-null float64\n",
      "Promo2_on_month_10     532356 non-null float64\n",
      "Promo2_on_month_11     532356 non-null float64\n",
      "Promo2_on_month_12     532356 non-null float64\n",
      "dtypes: datetime64[ns](3), float64(14), int64(5), object(3)\n",
      "memory usage: 209.7+ MB\n"
     ]
    }
   ],
   "source": [
    "combine.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill in missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in Open with mode\n",
    "combine['Open'] = combine.Open.fillna(1.0)\n",
    "\n",
    "# Fill in all the promo2 related attributes with -1, which indicates a special case(not participate in promo2)\n",
    "promo2_related_feats = [_ for _ in combine.columns if 'Promo2' in _]\n",
    "for feat in promo2_related_feats:\n",
    "    combine[feat] = combine[feat].fillna(-1)\n",
    "    \n",
    "# Fill in the missing values in CompetitionDistance with the max + 1000, which means no nearby competition\n",
    "combine['CompetitionDistance'] = combine['CompetitionDistance'].fillna(combine.CompetitionDistance.max() + 1000)\n",
    "\n",
    "# For the missing values in CompetitionOpen, I just chenge it to categorical attribute (with 0 means not open yet, 1 means opened, -1 means unclear)\n",
    "def convert_CompetitionOpen_cat(row):\n",
    "    if pd.isnull(row['CompetitionOpen']):\n",
    "        return -1\n",
    "    else:\n",
    "        if row['Date'] >= row['CompetitionOpen']:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "combine['CompetitionOpen'] = combine.apply(convert_CompetitionOpen_cat, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1057131 entries, 0 to 1057130\n",
      "Data columns (total 25 columns):\n",
      "Store                  1057131 non-null int64\n",
      "DayOfWeek              1057131 non-null int64\n",
      "Date                   1057131 non-null datetime64[ns]\n",
      "Open                   1057131 non-null float64\n",
      "Promo                  1057131 non-null int64\n",
      "StateHoliday           1057131 non-null object\n",
      "SchoolHoliday          1057131 non-null int64\n",
      "istestset              1057131 non-null int64\n",
      "StoreType              1057131 non-null object\n",
      "Assortment             1057131 non-null object\n",
      "CompetitionDistance    1057131 non-null float64\n",
      "CompetitionOpen        1057131 non-null int64\n",
      "Promo2Since            1057131 non-null object\n",
      "Promo2_on_month_1      1057131 non-null float64\n",
      "Promo2_on_month_2      1057131 non-null float64\n",
      "Promo2_on_month_3      1057131 non-null float64\n",
      "Promo2_on_month_4      1057131 non-null float64\n",
      "Promo2_on_month_5      1057131 non-null float64\n",
      "Promo2_on_month_6      1057131 non-null float64\n",
      "Promo2_on_month_7      1057131 non-null float64\n",
      "Promo2_on_month_8      1057131 non-null float64\n",
      "Promo2_on_month_9      1057131 non-null float64\n",
      "Promo2_on_month_10     1057131 non-null float64\n",
      "Promo2_on_month_11     1057131 non-null float64\n",
      "Promo2_on_month_12     1057131 non-null float64\n",
      "dtypes: datetime64[ns](1), float64(14), int64(6), object(4)\n",
      "memory usage: 209.7+ MB\n"
     ]
    }
   ],
   "source": [
    "combine.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DayOfWeek\n",
    "combine['DayOfWeek'] = combine.DayOfWeek.astype(int)\n",
    "\n",
    "# Date\n",
    "combine['Date'] = pd.to_datetime(combine.Date)\n",
    "combine['Year'] = combine.Date.dt.year.astype(int)\n",
    "combine['Month'] = combine.Date.dt.month.astype(int)\n",
    "combine['Day'] = combine.Date.dt.day.astype(int)\n",
    "combine['Week'] = combine.Date.dt.week.astype(int)\n",
    "\n",
    "# Open ...\n",
    "# Promo ...\n",
    "\n",
    "# StateHoliday\n",
    "combine['StateHoliday'] = combine.StateHoliday.replace({0:'0'}).astype('category').cat.codes\n",
    "\n",
    "# SchoolHoliday ...\n",
    "\n",
    "# StoreType\n",
    "combine['StoreType'] = combine.StoreType.astype('category').cat.codes\n",
    "\n",
    "# Assortment\n",
    "combine['Assortment'] = combine.Assortment.astype('category').cat.codes\n",
    "\n",
    "# Promo2 conver into catigorical feature(with 0 means not on, 1 means on, -1 means not participate)\n",
    "def create_promo2(row):\n",
    "    if row['Promo2Since'] == -1:\n",
    "        return -1\n",
    "    else:\n",
    "        month = row['Month']\n",
    "        if row['Promo2_on_month_' + str(month)] == 1 and row['Date'] > row['Promo2Since']:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "combine['Promo2'] = combine.apply(create_promo2, axis=1)\n",
    "combine = combine.drop(promo2_related_feats, axis=1)\n",
    "\n",
    "combine = combine.drop('Date', axis=1)\n",
    "\n",
    "combine = pd.get_dummies(combine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1057131 entries, 0 to 1057130\n",
      "Data columns (total 16 columns):\n",
      "Store                  1057131 non-null int64\n",
      "DayOfWeek              1057131 non-null int64\n",
      "Open                   1057131 non-null float64\n",
      "Promo                  1057131 non-null int64\n",
      "StateHoliday           1057131 non-null int8\n",
      "SchoolHoliday          1057131 non-null int64\n",
      "istestset              1057131 non-null int64\n",
      "StoreType              1057131 non-null int8\n",
      "Assortment             1057131 non-null int8\n",
      "CompetitionDistance    1057131 non-null float64\n",
      "CompetitionOpen        1057131 non-null int64\n",
      "Year                   1057131 non-null int64\n",
      "Month                  1057131 non-null int64\n",
      "Day                    1057131 non-null int64\n",
      "Week                   1057131 non-null int64\n",
      "Promo2                 1057131 non-null int64\n",
      "dtypes: float64(2), int64(11), int8(3)\n",
      "memory usage: 115.9 MB\n"
     ]
    }
   ],
   "source": [
    "combine.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the data for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = combine[combine.istestset==0].drop('istestset', axis=1)\n",
    "label_train = train['Sales']\n",
    "label_train_log = np.log1p(train['Sales'])\n",
    "data_test = combine[combine.istestset==1].drop('istestset', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the basic setting of the model and Set up the metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from lightgbm.sklearn import LGBMRegressor\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import make_scorer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I tried the following 4 settings for the model and choose the best:\n",
    "\n",
    "- Default objective function without taking log of the target\n",
    "- Default objective function with taking log of the target\n",
    "- Custom objective function without taking log of the target\n",
    "- Custom objective function with taking log of the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_eval, y_train, y_eval = train_test_split(data_train, label_train, test_size=0.1, random_state=0)\n",
    "X_train, X_eval, y_train_log, y_eval_log = train_test_split(data_train, label_train_log, test_size=0.1, random_state=0)\n",
    "\n",
    "dataset_train = lgb.Dataset(X_train, y_train)\n",
    "dataset_eval = lgb.Dataset(X_eval, y_eval, reference=dataset_train)\n",
    "\n",
    "dataset_train_log = lgb.Dataset(X_train, y_train_log)\n",
    "dataset_eval_log = lgb.Dataset(X_eval, y_eval_log, reference=dataset_train_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rounds = 2000\n",
    "verb_rounds = 100\n",
    "earlystop_rounds = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default objective function without taking log of the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttraining set's rmspe: 0.353539\tvalidation set's rmspe: 0.328529\n",
      "[200]\ttraining set's rmspe: 0.294888\tvalidation set's rmspe: 0.268289\n",
      "[300]\ttraining set's rmspe: 0.260906\tvalidation set's rmspe: 0.234064\n",
      "[400]\ttraining set's rmspe: 0.241588\tvalidation set's rmspe: 0.213323\n",
      "[500]\ttraining set's rmspe: 0.226319\tvalidation set's rmspe: 0.197813\n",
      "[600]\ttraining set's rmspe: 0.217571\tvalidation set's rmspe: 0.189241\n",
      "[700]\ttraining set's rmspe: 0.211076\tvalidation set's rmspe: 0.181455\n",
      "[800]\ttraining set's rmspe: 0.205693\tvalidation set's rmspe: 0.175445\n",
      "[900]\ttraining set's rmspe: 0.200428\tvalidation set's rmspe: 0.1696\n",
      "[1000]\ttraining set's rmspe: 0.19638\tvalidation set's rmspe: 0.165255\n",
      "[1100]\ttraining set's rmspe: 0.19253\tvalidation set's rmspe: 0.16175\n",
      "[1200]\ttraining set's rmspe: 0.190344\tvalidation set's rmspe: 0.159234\n",
      "[1300]\ttraining set's rmspe: 0.188339\tvalidation set's rmspe: 0.157061\n",
      "[1400]\ttraining set's rmspe: 0.185443\tvalidation set's rmspe: 0.154303\n",
      "[1500]\ttraining set's rmspe: 0.183394\tvalidation set's rmspe: 0.15226\n",
      "[1600]\ttraining set's rmspe: 0.181671\tvalidation set's rmspe: 0.15063\n",
      "[1700]\ttraining set's rmspe: 0.17981\tvalidation set's rmspe: 0.149895\n",
      "[1800]\ttraining set's rmspe: 0.178757\tvalidation set's rmspe: 0.148736\n",
      "[1900]\ttraining set's rmspe: 0.17785\tvalidation set's rmspe: 0.147365\n",
      "[2000]\ttraining set's rmspe: 0.175992\tvalidation set's rmspe: 0.146256\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2000]\ttraining set's rmspe: 0.175992\tvalidation set's rmspe: 0.146256\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<lightgbm.basic.Booster at 0x7ff4d8e7d630>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def loss(y_pred, y):\n",
    "    y = y.get_label()\n",
    "    n = y.shape[0]\n",
    "    ind = (y!=0)\n",
    "    y_pred, y = y_pred[ind], y[ind]\n",
    "    dis_percent_squared = ((y - y_pred) / y) ** 2\n",
    "    loss = np.sqrt(dis_percent_squared.sum() / n)\n",
    "    return 'rmspe', loss, False\n",
    "\n",
    "lgb.train({}, dataset_train, num_rounds, \n",
    "          valid_sets=[dataset_train, dataset_eval], valid_names=['training set', 'validation set'], \n",
    "          feval=loss, verbose_eval=verb_rounds, early_stopping_rounds=earlystop_rounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default objective function with taking log of the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttraining set's rmspe_log: 0.288466\tvalidation set's rmspe_log: 0.270907\n",
      "[200]\ttraining set's rmspe_log: 0.244215\tvalidation set's rmspe_log: 0.222901\n",
      "[300]\ttraining set's rmspe_log: 0.218444\tvalidation set's rmspe_log: 0.195883\n",
      "[400]\ttraining set's rmspe_log: 0.204562\tvalidation set's rmspe_log: 0.180372\n",
      "[500]\ttraining set's rmspe_log: 0.194914\tvalidation set's rmspe_log: 0.170031\n",
      "[600]\ttraining set's rmspe_log: 0.186844\tvalidation set's rmspe_log: 0.162092\n",
      "[700]\ttraining set's rmspe_log: 0.181111\tvalidation set's rmspe_log: 0.155561\n",
      "[800]\ttraining set's rmspe_log: 0.177329\tvalidation set's rmspe_log: 0.151472\n",
      "[900]\ttraining set's rmspe_log: 0.173705\tvalidation set's rmspe_log: 0.14815\n",
      "[1000]\ttraining set's rmspe_log: 0.168835\tvalidation set's rmspe_log: 0.144647\n",
      "[1100]\ttraining set's rmspe_log: 0.16548\tvalidation set's rmspe_log: 0.142427\n",
      "[1200]\ttraining set's rmspe_log: 0.162481\tvalidation set's rmspe_log: 0.140195\n",
      "[1300]\ttraining set's rmspe_log: 0.160287\tvalidation set's rmspe_log: 0.138729\n",
      "[1400]\ttraining set's rmspe_log: 0.157999\tvalidation set's rmspe_log: 0.137242\n",
      "[1500]\ttraining set's rmspe_log: 0.156327\tvalidation set's rmspe_log: 0.135985\n",
      "[1600]\ttraining set's rmspe_log: 0.154079\tvalidation set's rmspe_log: 0.134373\n",
      "[1700]\ttraining set's rmspe_log: 0.151471\tvalidation set's rmspe_log: 0.133474\n",
      "[1800]\ttraining set's rmspe_log: 0.148071\tvalidation set's rmspe_log: 0.132366\n",
      "[1900]\ttraining set's rmspe_log: 0.147038\tvalidation set's rmspe_log: 0.131355\n",
      "[2000]\ttraining set's rmspe_log: 0.146299\tvalidation set's rmspe_log: 0.130339\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2000]\ttraining set's rmspe_log: 0.146299\tvalidation set's rmspe_log: 0.130339\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<lightgbm.basic.Booster at 0x7ff4d8e90128>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def loss_log(y_pred, y):\n",
    "    y = y.get_label()\n",
    "    n = y.shape[0]\n",
    "    y_pred = np.exp(y_pred) - 1\n",
    "    y = np.exp(y) - 1\n",
    "    ind = (y!=0)\n",
    "    y_pred, y = y_pred[ind], y[ind]\n",
    "    dis_percent_squared = ((y - y_pred) / y) ** 2\n",
    "    loss = np.sqrt(dis_percent_squared.sum() / n)\n",
    "    return 'rmspe_log', loss, False\n",
    "\n",
    "lgb.train({}, dataset_train_log, num_rounds, \n",
    "          valid_sets = [dataset_train_log, dataset_eval_log], valid_names=['training set', 'validation set'], \n",
    "          feval=loss_log, verbose_eval=verb_rounds, early_stopping_rounds=earlystop_rounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom objective function without taking log of the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttraining set's rmspe: 0.263805\tvalidation set's rmspe: 0.266756\n",
      "[200]\ttraining set's rmspe: 0.237106\tvalidation set's rmspe: 0.242441\n",
      "[300]\ttraining set's rmspe: 0.221949\tvalidation set's rmspe: 0.227956\n",
      "[400]\ttraining set's rmspe: 0.211457\tvalidation set's rmspe: 0.21837\n",
      "[500]\ttraining set's rmspe: 0.203762\tvalidation set's rmspe: 0.211221\n",
      "[600]\ttraining set's rmspe: 0.197416\tvalidation set's rmspe: 0.205294\n",
      "[700]\ttraining set's rmspe: 0.192239\tvalidation set's rmspe: 0.200601\n",
      "[800]\ttraining set's rmspe: 0.187602\tvalidation set's rmspe: 0.196257\n",
      "[900]\ttraining set's rmspe: 0.183606\tvalidation set's rmspe: 0.192457\n",
      "[1000]\ttraining set's rmspe: 0.180059\tvalidation set's rmspe: 0.189421\n",
      "[1100]\ttraining set's rmspe: 0.177011\tvalidation set's rmspe: 0.18651\n",
      "[1200]\ttraining set's rmspe: 0.174244\tvalidation set's rmspe: 0.183894\n",
      "[1300]\ttraining set's rmspe: 0.17162\tvalidation set's rmspe: 0.181496\n",
      "[1400]\ttraining set's rmspe: 0.169223\tvalidation set's rmspe: 0.179328\n",
      "[1500]\ttraining set's rmspe: 0.167068\tvalidation set's rmspe: 0.177299\n",
      "[1600]\ttraining set's rmspe: 0.165168\tvalidation set's rmspe: 0.175633\n",
      "[1700]\ttraining set's rmspe: 0.163341\tvalidation set's rmspe: 0.173942\n",
      "[1800]\ttraining set's rmspe: 0.161698\tvalidation set's rmspe: 0.172382\n",
      "[1900]\ttraining set's rmspe: 0.160066\tvalidation set's rmspe: 0.170981\n",
      "[2000]\ttraining set's rmspe: 0.15853\tvalidation set's rmspe: 0.169646\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2000]\ttraining set's rmspe: 0.15853\tvalidation set's rmspe: 0.169646\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<lightgbm.basic.Booster at 0x7ff4da840e48>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def rmspe_loss(y_pred, y):\n",
    "    y = y.get_label()\n",
    "    dis = y - y_pred\n",
    "    ind = (dis!=0) & (y!=0)\n",
    "    gradient = np.zeros(y.shape)\n",
    "    hession = np.zeros(y.shape)\n",
    "    n = y.shape[0]\n",
    "    gradient[ind] = 2 * (y_pred[ind] - y[ind]) / y[ind] ** 2\n",
    "    hession[ind] = 2 / y[ind] ** 2\n",
    "    return gradient, hession\n",
    "\n",
    "lgb.train({}, dataset_train, num_rounds, \n",
    "          valid_sets = [dataset_train, dataset_eval], \n",
    "          valid_names=['training set', 'validation set'], \n",
    "          fobj=rmspe_loss, feval=loss, \n",
    "          verbose_eval=verb_rounds, early_stopping_rounds=earlystop_rounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom objective function with taking log of the target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\ttraining set's rmspe_log: 0.272429\tvalidation set's rmspe_log: 0.253568\n",
      "[200]\ttraining set's rmspe_log: 0.223255\tvalidation set's rmspe_log: 0.200649\n",
      "[300]\ttraining set's rmspe_log: 0.201062\tvalidation set's rmspe_log: 0.176837\n",
      "[400]\ttraining set's rmspe_log: 0.176031\tvalidation set's rmspe_log: 0.163059\n",
      "[500]\ttraining set's rmspe_log: 0.162309\tvalidation set's rmspe_log: 0.154052\n",
      "[600]\ttraining set's rmspe_log: 0.155076\tvalidation set's rmspe_log: 0.147874\n",
      "[700]\ttraining set's rmspe_log: 0.146765\tvalidation set's rmspe_log: 0.143103\n",
      "[800]\ttraining set's rmspe_log: 0.137531\tvalidation set's rmspe_log: 0.139298\n",
      "[900]\ttraining set's rmspe_log: 0.131938\tvalidation set's rmspe_log: 0.137782\n",
      "[1000]\ttraining set's rmspe_log: 0.127034\tvalidation set's rmspe_log: 0.136196\n",
      "[1100]\ttraining set's rmspe_log: 0.121354\tvalidation set's rmspe_log: 0.134522\n",
      "[1200]\ttraining set's rmspe_log: 0.116847\tvalidation set's rmspe_log: 0.132746\n",
      "[1300]\ttraining set's rmspe_log: 0.113769\tvalidation set's rmspe_log: 0.13124\n",
      "[1400]\ttraining set's rmspe_log: 0.111347\tvalidation set's rmspe_log: 0.13031\n",
      "[1500]\ttraining set's rmspe_log: 0.109443\tvalidation set's rmspe_log: 0.129468\n",
      "[1600]\ttraining set's rmspe_log: 0.108136\tvalidation set's rmspe_log: 0.128519\n",
      "[1700]\ttraining set's rmspe_log: 0.106711\tvalidation set's rmspe_log: 0.12807\n",
      "[1800]\ttraining set's rmspe_log: 0.105072\tvalidation set's rmspe_log: 0.12749\n",
      "[1900]\ttraining set's rmspe_log: 0.103483\tvalidation set's rmspe_log: 0.126873\n",
      "[2000]\ttraining set's rmspe_log: 0.102408\tvalidation set's rmspe_log: 0.126228\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2000]\ttraining set's rmspe_log: 0.102408\tvalidation set's rmspe_log: 0.126228\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<lightgbm.basic.Booster at 0x7ff4d8e5e470>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def rmspe_loss(y_pred, y):\n",
    "    y = y.get_label()\n",
    "    dis = y - y_pred\n",
    "    ind = (dis!=0) & (y!=0)\n",
    "    gradient = np.zeros(y.shape)\n",
    "    hession = np.zeros(y.shape)\n",
    "    n = y.shape[0]\n",
    "    gradient[ind] = 2 * (y_pred[ind] - y[ind]) / y[ind] ** 2\n",
    "    hession[ind] = 2 / y[ind] ** 2\n",
    "    return gradient, hession\n",
    "\n",
    "lgb.train({}, dataset_train_log, num_rounds, \n",
    "          valid_sets = [dataset_train_log, dataset_eval_log], \n",
    "          valid_names=['training set', 'validation set'], \n",
    "          fobj=rmspe_loss, feval=loss_log, \n",
    "          verbose_eval=verb_rounds, early_stopping_rounds=earlystop_rounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Since the custom objective function with taking log performs the best, I choose this setting**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gridsearch for best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KFold for consistant cv:\n",
    "kf = KFold(n_splits=4, shuffle=True, random_state=0)\n",
    "\n",
    "# Custom scoring function\n",
    "def loss_log_sk(y, y_pred):\n",
    "    n = y.shape[0]\n",
    "    y_pred = np.exp(y_pred) - 1\n",
    "    y = np.exp(y) - 1\n",
    "    ind = (y!=0)\n",
    "    y_pred, y = y_pred[ind], y[ind]\n",
    "    dis_percent_squared = ((y - y_pred) / y) ** 2\n",
    "    loss = np.sqrt(dis_percent_squared.sum() / n)\n",
    "    return loss\n",
    "mt = make_scorer(loss_log_sk, greater_is_better=False)\n",
    "\n",
    "# Simple performance measure function\n",
    "def performance(model):\n",
    "    scores = cross_val_score(model, data_train, label_train_log, cv=kf, scoring=mt, n_jobs=4, verbose=True)\n",
    "    score_mean = scores.mean()\n",
    "    score_std = scores.std()\n",
    "    print('score mean: {}'.format(-score_mean))\n",
    "    print('score std: {}'.format(score_std))\n",
    "\n",
    "# Grid search function\n",
    "def grid_search(model, params):\n",
    "    grid = GridSearchCV(model, params, cv=kf, scoring=mt, n_jobs=4, verbose=True).fit(data_train, label_train_log)\n",
    "    print('grid.best_score_: {}'.format(-grid.best_score_))\n",
    "    print('grid.best_params_: \\n{}'.format(grid.best_params_))\n",
    "    return grid.best_estimator_\n",
    "\n",
    "# Custom objective function for sklearn api\n",
    "def rmspe_loss_sk(y, y_pred):\n",
    "    dis = y - y_pred\n",
    "    ind = (dis!=0) & (y!=0)\n",
    "    gradient = np.zeros(y.shape)\n",
    "    hession = np.zeros(y.shape)\n",
    "    n = y.shape[0]\n",
    "    gradient[ind] = 2 * (y_pred[ind] - y[ind]) / y[ind] ** 2\n",
    "    hession[ind] = 2 / y[ind] ** 2\n",
    "    return gradient, hession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score mean: 0.28628034827048887\n",
      "score std: 0.014942754499771175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done   2 out of   4 | elapsed:   20.5s remaining:   20.5s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:   20.6s finished\n"
     ]
    }
   ],
   "source": [
    "performance(LGBMRegressor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   2 out of   4 | elapsed:   28.6s remaining:   28.6s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score mean: 0.2707011153408984\n",
      "score std: 0.017801804979838853\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:   29.3s finished\n"
     ]
    }
   ],
   "source": [
    "performance(LGBMRegressor(objective=rmspe_loss_sk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 1 candidates, totalling 4 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done   2 out of   4 | elapsed:   42.5s remaining:   42.5s\n",
      "[Parallel(n_jobs=4)]: Done   4 out of   4 | elapsed:   42.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grid.best_score_: 0.15112112057618407\n",
      "grid.best_params_: \n",
      "{'colsample_bytree': 0.9, 'learning_rate': 0.6, 'max_depth': 10, 'n_estimators': 100, 'num_leaves': 610, 'objective': <function rmspe_loss_sk at 0x7ff4d8e63510>, 'random_state': 0, 'reg_alpha': 0, 'reg_lambda': 0, 'subsample': 0.1}\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    'random_state': [0],\n",
    "    'n_estimators': [100],\n",
    "    'learning_rate': [0.6],\n",
    "    'objective': [rmspe_loss_sk],\n",
    "    'num_leaves': [610],\n",
    "    'max_depth': [10],\n",
    "    'reg_alpha': [0],\n",
    "    'reg_lambda': [0],\n",
    "    'subsample': [0.1],\n",
    "    'colsample_bytree': [0.9]\n",
    "}\n",
    "reg = grid_search(LGBMRegressor(), params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set small learning_rate, then use early stopping to find the optimal iteration rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 250 rounds.\n",
      "[250]\ttraining set's rmspe_log: 0.47528\tvalidation set's rmspe_log: 0.473774\n",
      "[500]\ttraining set's rmspe_log: 0.211774\tvalidation set's rmspe_log: 0.209392\n",
      "[750]\ttraining set's rmspe_log: 0.181635\tvalidation set's rmspe_log: 0.184329\n",
      "[1000]\ttraining set's rmspe_log: 0.155773\tvalidation set's rmspe_log: 0.164762\n",
      "[1250]\ttraining set's rmspe_log: 0.134726\tvalidation set's rmspe_log: 0.149248\n",
      "[1500]\ttraining set's rmspe_log: 0.122951\tvalidation set's rmspe_log: 0.14097\n",
      "[1750]\ttraining set's rmspe_log: 0.115119\tvalidation set's rmspe_log: 0.136154\n",
      "[2000]\ttraining set's rmspe_log: 0.108949\tvalidation set's rmspe_log: 0.132017\n",
      "[2250]\ttraining set's rmspe_log: 0.104433\tvalidation set's rmspe_log: 0.129067\n",
      "[2500]\ttraining set's rmspe_log: 0.100758\tvalidation set's rmspe_log: 0.126853\n",
      "[2750]\ttraining set's rmspe_log: 0.098039\tvalidation set's rmspe_log: 0.125481\n",
      "[3000]\ttraining set's rmspe_log: 0.0955919\tvalidation set's rmspe_log: 0.12401\n",
      "[3250]\ttraining set's rmspe_log: 0.093297\tvalidation set's rmspe_log: 0.123113\n",
      "[3500]\ttraining set's rmspe_log: 0.0914006\tvalidation set's rmspe_log: 0.122239\n",
      "[3750]\ttraining set's rmspe_log: 0.0894443\tvalidation set's rmspe_log: 0.121387\n",
      "[4000]\ttraining set's rmspe_log: 0.0879751\tvalidation set's rmspe_log: 0.120579\n",
      "[4250]\ttraining set's rmspe_log: 0.0866566\tvalidation set's rmspe_log: 0.119515\n",
      "[4500]\ttraining set's rmspe_log: 0.0853428\tvalidation set's rmspe_log: 0.11839\n",
      "[4750]\ttraining set's rmspe_log: 0.0841508\tvalidation set's rmspe_log: 0.117802\n",
      "[5000]\ttraining set's rmspe_log: 0.0831121\tvalidation set's rmspe_log: 0.117204\n",
      "[5250]\ttraining set's rmspe_log: 0.0823093\tvalidation set's rmspe_log: 0.116932\n",
      "[5500]\ttraining set's rmspe_log: 0.0815674\tvalidation set's rmspe_log: 0.116728\n",
      "[5750]\ttraining set's rmspe_log: 0.0807852\tvalidation set's rmspe_log: 0.116406\n",
      "[6000]\ttraining set's rmspe_log: 0.0800558\tvalidation set's rmspe_log: 0.116126\n",
      "[6250]\ttraining set's rmspe_log: 0.0793436\tvalidation set's rmspe_log: 0.115407\n",
      "[6500]\ttraining set's rmspe_log: 0.0786982\tvalidation set's rmspe_log: 0.11509\n",
      "[6750]\ttraining set's rmspe_log: 0.0781613\tvalidation set's rmspe_log: 0.115008\n",
      "[7000]\ttraining set's rmspe_log: 0.0775993\tvalidation set's rmspe_log: 0.114527\n",
      "[7250]\ttraining set's rmspe_log: 0.0770203\tvalidation set's rmspe_log: 0.114211\n",
      "[7500]\ttraining set's rmspe_log: 0.0764444\tvalidation set's rmspe_log: 0.114066\n",
      "[7750]\ttraining set's rmspe_log: 0.075906\tvalidation set's rmspe_log: 0.113901\n",
      "[8000]\ttraining set's rmspe_log: 0.0754605\tvalidation set's rmspe_log: 0.113712\n",
      "[8250]\ttraining set's rmspe_log: 0.074986\tvalidation set's rmspe_log: 0.113595\n",
      "[8500]\ttraining set's rmspe_log: 0.074499\tvalidation set's rmspe_log: 0.113426\n",
      "[8750]\ttraining set's rmspe_log: 0.0739653\tvalidation set's rmspe_log: 0.113339\n",
      "[9000]\ttraining set's rmspe_log: 0.0735603\tvalidation set's rmspe_log: 0.113225\n",
      "[9250]\ttraining set's rmspe_log: 0.0730836\tvalidation set's rmspe_log: 0.112891\n",
      "[9500]\ttraining set's rmspe_log: 0.0725195\tvalidation set's rmspe_log: 0.112683\n",
      "[9750]\ttraining set's rmspe_log: 0.07217\tvalidation set's rmspe_log: 0.112717\n",
      "Early stopping, best iteration is:\n",
      "[9617]\ttraining set's rmspe_log: 0.0723262\tvalidation set's rmspe_log: 0.11264\n"
     ]
    }
   ],
   "source": [
    "num_rounds = 20000\n",
    "verb_rounds = 250\n",
    "earlystop_rounds = 250\n",
    "\n",
    "params = {\n",
    "    'random_state': [0],\n",
    "    'learning_rate': [0.01],\n",
    "    'num_leaves': [610],\n",
    "    'max_depth': [10],\n",
    "    'reg_alpha': [0],\n",
    "    'reg_lambda': [0],\n",
    "    'subsample': [0.1],\n",
    "    'colsample_bytree': [0.9]\n",
    "}\n",
    "\n",
    "reg = lgb.train(params, dataset_train_log, num_rounds,\n",
    "                valid_sets=[dataset_train_log, dataset_eval_log],\n",
    "                valid_names=['training set', 'validation set'],\n",
    "                fobj=rmspe_loss, feval=loss_log,\n",
    "                verbose_eval=verb_rounds, early_stopping_rounds=earlystop_rounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrain the model on the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 250 rounds.\n",
      "[250]\ttraining set's rmspe_log: 0.475483\n",
      "[500]\ttraining set's rmspe_log: 0.223541\n",
      "[750]\ttraining set's rmspe_log: 0.183736\n",
      "[1000]\ttraining set's rmspe_log: 0.155916\n",
      "[1250]\ttraining set's rmspe_log: 0.135365\n",
      "[1500]\ttraining set's rmspe_log: 0.123902\n",
      "[1750]\ttraining set's rmspe_log: 0.115415\n",
      "[2000]\ttraining set's rmspe_log: 0.109643\n",
      "[2250]\ttraining set's rmspe_log: 0.104672\n",
      "[2500]\ttraining set's rmspe_log: 0.101247\n",
      "[2750]\ttraining set's rmspe_log: 0.0986299\n",
      "[3000]\ttraining set's rmspe_log: 0.0962704\n",
      "[3250]\ttraining set's rmspe_log: 0.0940317\n",
      "[3500]\ttraining set's rmspe_log: 0.0921822\n",
      "[3750]\ttraining set's rmspe_log: 0.0903653\n",
      "[4000]\ttraining set's rmspe_log: 0.0892545\n",
      "[4250]\ttraining set's rmspe_log: 0.0877802\n",
      "[4500]\ttraining set's rmspe_log: 0.0863914\n",
      "[4750]\ttraining set's rmspe_log: 0.0852937\n",
      "[5000]\ttraining set's rmspe_log: 0.0843493\n",
      "[5250]\ttraining set's rmspe_log: 0.0834513\n",
      "[5500]\ttraining set's rmspe_log: 0.0825009\n",
      "[5750]\ttraining set's rmspe_log: 0.0818723\n",
      "[6000]\ttraining set's rmspe_log: 0.0811741\n",
      "[6250]\ttraining set's rmspe_log: 0.0803431\n",
      "[6500]\ttraining set's rmspe_log: 0.0797471\n",
      "[6750]\ttraining set's rmspe_log: 0.0791666\n",
      "[7000]\ttraining set's rmspe_log: 0.0785221\n",
      "[7250]\ttraining set's rmspe_log: 0.0779267\n",
      "[7500]\ttraining set's rmspe_log: 0.0773586\n",
      "[7750]\ttraining set's rmspe_log: 0.0769284\n",
      "[8000]\ttraining set's rmspe_log: 0.0764258\n",
      "[8250]\ttraining set's rmspe_log: 0.0758102\n",
      "[8500]\ttraining set's rmspe_log: 0.0753633\n",
      "[8750]\ttraining set's rmspe_log: 0.0748647\n",
      "[9000]\ttraining set's rmspe_log: 0.0743759\n",
      "[9250]\ttraining set's rmspe_log: 0.0739594\n",
      "[9500]\ttraining set's rmspe_log: 0.0735498\n",
      "[9750]\ttraining set's rmspe_log: 0.0731069\n",
      "[10000]\ttraining set's rmspe_log: 0.0727015\n",
      "[10250]\ttraining set's rmspe_log: 0.0723465\n",
      "[10500]\ttraining set's rmspe_log: 0.0719849\n",
      "[10750]\ttraining set's rmspe_log: 0.0716743\n",
      "[11000]\ttraining set's rmspe_log: 0.0713249\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[11000]\ttraining set's rmspe_log: 0.0713249\n"
     ]
    }
   ],
   "source": [
    "dataset = lgb.Dataset(data_train, label_train_log)\n",
    "reg = lgb.train(params, dataset, 11000,\n",
    "                valid_sets=[dataset],\n",
    "                valid_names=['training set'],\n",
    "                fobj=rmspe_loss, feval=loss_log, \n",
    "                verbose_eval=verb_rounds, early_stopping_rounds=earlystop_rounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7ff4f259d7b8>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=reg.feature_importance(), y=reg.feature_name())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "y_pred = np.exp(reg.predict(data_test)) - 1\n",
    "y_pred = pd.Series(y_pred, name='Sales')\n",
    "sub = pd.concat([test['Id'], y_pred], axis=1)\n",
    "sub.to_csv('sub.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
